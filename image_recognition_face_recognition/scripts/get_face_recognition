#!/usr/bin/env python
from __future__ import print_function

import argparse

import cv2
from image_recognition_msgs.msg import Recognition
from image_recognition_face_recognition.facenet_recognition import Facenet_recognition
# Assign description to the help doc
import math
from sensor_msgs.msg import RegionOfInterest

from image_recognition_util import image_writer

parser = argparse.ArgumentParser(description='Get face recognitions')

# Add arguments
parser.add_argument('image', type=str, help='Image')
parser.add_argument('-k', '--align_path', type=str, help='DLib Align path', required=False,
                    default="~/openface/models/dlib/shape_predictor_68_face_landmarks.dat")
parser.add_argument('-s', '--net_path', type=str, help='Openface neural network path', required=False,
                    default='~/openface/models/openface/nn4.small2.v1.t7')
parser.add_argument('-d', '--db', type=argparse.FileType('r'), help='Load already trained faces db from file')
parser.add_argument(
    '-v', '--verbose', help="Increase output verbosity", action="store_true")
args = parser.parse_args()

# Read the image
img = cv2.imread(args.image)

# Create openface interface
face_recognizer = Facenet_recognition()

if args.db:
    face_recognizer.restore_trained_faces(args.db)

recognized_faces = face_recognizer.face_detection(img)
print(recognized_faces)

recognitions = []
for fr in recognized_faces:
    face_recognition = [math.floor(xi) for xi in fr]
    recognitions.append(Recognition(
        roi=RegionOfInterest(
            x_offset=face_recognition[0],
            y_offset=face_recognition[1],
            width=face_recognition[2] - face_recognition[0],
            height=face_recognition[3] - face_recognition[1],
        )
    )
)
    annotated_original_image = image_writer.get_annotated_cv_image(img, recognitions) 
cv2.imshow("result", annotated_original_image)
cv2.waitKey()
